{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer,Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_training_samples(file_names,debug=False):\n",
    "    num_train_samples = 0\n",
    "    for file_name in file_names:\n",
    "        x_train,y_train = read_numpy_file(file_name)\n",
    "        assert len(x_train) == len(y_train), 'Number of samples must be same'\n",
    "        if debug:\n",
    "            print('X samples:{},Y samples:{}'.format(len(x_train),len(y_train)))\n",
    "        num_train_samples = num_train_samples + len(x_train)\n",
    "    return num_train_samples\n",
    "        \n",
    "def read_numpy_file(file_name):\n",
    "    \n",
    "    data = np.load(os.path.join(folder_name, file_name))\n",
    "    #data = list(data)\n",
    "    x_train = np.array([i[1] for i in data[:-10]])\n",
    "    y_train = np.array([i[0] for i in data[:-10]])\n",
    "    return x_train,y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 2868\n",
      "Number of training files: 2294\n",
      "Number of validation files: 574\n"
     ]
    }
   ],
   "source": [
    "folder_name_data = \"./training_data_stage1\"\n",
    "folder_name_models = \"./keras_models\"\n",
    "#folder_name = \"./training_data_own\"\n",
    "file_names = os.listdir(folder_name_data)\n",
    "random.shuffle(file_names)\n",
    "file_names_training = file_names[0:int(len(file_names)*0.8)]\n",
    "file_names_validation = file_names[-(len(file_names)-int(len(file_names)*0.8)):]\n",
    "#num_train_samples = count_training_samples(file_names)\n",
    "print('Number of files:',len(file_names))\n",
    "print('Number of training files:',len(file_names_training))\n",
    "print('Number of validation files:',len(file_names_validation))\n",
    "#print('Total training examples:',num_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to parse files for data (can be used for both training and validation data)\n",
    "def _get_data_for_dataset(file_name):\n",
    "    #print(file_name.decode())\n",
    "    data = np.load(os.path.join(folder_name_data, file_name.decode()))\n",
    "    #data = list(data)\n",
    "    \n",
    "    x_train = np.array([i[1] for i in data[:-10]]).reshape(-1, 176, 200, 3)\n",
    "    y_train = np.array([i[0] for i in data[:-10]])\n",
    "    return (x_train/255).astype('float32', copy=False),y_train.astype('float32', copy=False)\n",
    "\n",
    "#Make dataset for training\n",
    "dataset = tf.data.Dataset.from_tensor_slices(file_names_training)\n",
    "dataset = dataset.flat_map(lambda file_name: tf.data.Dataset.from_tensor_slices(\n",
    "    tuple (tf.py_func(_get_data_for_dataset, [file_name], [tf.float32,tf.float32]))))\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=200)\n",
    "dataset = dataset.batch(20) #Make dataset, shuffle, and create batches\n",
    "#dataset= dataset.prefetch(1) \n",
    "dataset = dataset.repeat()\n",
    "\n",
    "dataset_iterator = dataset.make_one_shot_iterator()\n",
    "get_batch = dataset_iterator.get_next()\n",
    "\n",
    "#Make dataset for validation\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices(file_names_validation)\n",
    "dataset_valid = dataset_valid.flat_map(lambda file_name: tf.data.Dataset.from_tensor_slices(\n",
    "    tuple (tf.py_func(_get_data_for_dataset, [file_name], [tf.float32,tf.float32]))))\n",
    "\n",
    "dataset_valid = dataset_valid.shuffle(buffer_size=200)\n",
    "dataset_valid = dataset_valid.batch(20) #Make dataset, shuffle, and create batches\n",
    "#dataset= dataset.prefetch(1) \n",
    "dataset_valid = dataset_valid.repeat()\n",
    "\n",
    "dataset_valid_iterator = dataset.make_one_shot_iterator()\n",
    "get_batch_valid = dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing model named:sequential\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 176, 200, 32)      896       \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 174, 198, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 87, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 87, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 87, 99, 64)        18496     \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 85, 97, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 42, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv2D)               (None, 42, 48, 128)       73856     \n",
      "_________________________________________________________________\n",
      "Conv6 (Conv2D)               (None, 40, 46, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 58880)             0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 512)               30147072  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 30,436,132\n",
      "Trainable params: 30,436,132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'model.12-0.82.hdf5'\n",
    "USE_EXISTING_MODEL = True\n",
    "if os.path.isfile(os.path.join(folder_name_models, model_file_name)) and USE_EXISTING_MODEL == True:\n",
    "    \n",
    "    model =  tf.keras.models.load_model(os.path.join(folder_name_models, model_file_name))\n",
    "    print('Loaded existing model named:{}'.format(model.name))\n",
    "else:\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(InputLayer(input_shape=(176,200,3),name='Input'))\n",
    "    model.add(Conv2D(32,(3,3),padding='same',input_shape=(176,200,3),activation ='relu',name='Conv1'))\n",
    "    model.add(Conv2D(32,(3,3),activation ='relu',name='Conv2'))              #Reduces size\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64,(3,3),padding='same',activation ='relu',name='Conv3'))\n",
    "    model.add(Conv2D(64,(3,3),activation ='relu',name='Conv4'))           #Reduces size\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128,(3,3),padding='same',activation ='relu',name='Conv5'))\n",
    "    model.add(Conv2D(128,(3,3),activation ='relu',name='Conv6'))           #Reduces size\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation = 'relu',name='Dense1'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(4,activation = 'softmax',name='Output'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate,decay=1e-6)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer = optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    print('Created new model')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs/stage1')\n",
    "checkpointer = ModelCheckpoint(filepath='./keras_models/model.{epoch:02d}-{loss:.3f}-{acc:.2f}-{val_acc:.2f}.hdf5',monitor='acc',period=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 2041s 408ms/step - loss: 0.8989 - acc: 0.6257 - val_loss: 1.0879 - val_acc: 0.5512\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 2002s 400ms/step - loss: 0.8092 - acc: 0.6687 - val_loss: 1.1108 - val_acc: 0.5313\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 1998s 400ms/step - loss: 0.7021 - acc: 0.7175 - val_loss: 1.1647 - val_acc: 0.5125\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 1997s 399ms/step - loss: 0.6153 - acc: 0.7580 - val_loss: 1.3194 - val_acc: 0.4955\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 1994s 399ms/step - loss: 0.5438 - acc: 0.7902 - val_loss: 1.3615 - val_acc: 0.4810\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=5000,validation_data=dataset_valid,validation_steps =1000,verbose=1, callbacks=[tensorboard,checkpointer])\n",
    "model.save(\"BasicCNN-{}-epochs-{}-LR-STAGE1\".format(epochs, learning_rate))\n",
    "print('Model saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
